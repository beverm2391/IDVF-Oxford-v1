{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Union\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from lib.utils import get_93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_preprocess(data: pd.DataFrame, windsorize=True):\n",
    "    \"\"\"\n",
    "    Handle missing values, convert datetime to index, windsorize outliers, rename columns\n",
    "    \"\"\"\n",
    "    data = data.ffill()\n",
    "    data = data.bfill()\n",
    "    assert data.isna().sum().sum() == 0\n",
    "\n",
    "    data['datetime'] = pd.to_datetime(data['datetime'], utc=True).dt.tz_convert('US/Eastern')\n",
    "    data.set_index('datetime', inplace=True)\n",
    "\n",
    "    data.columns = [col for col in data.columns if col != 'datetime'] # filter out datetime\n",
    "    namelist = data.columns.tolist() # save column names (before renaming)\n",
    "    data.columns = [col + '_logvol' for col in data.columns] # rename columns\n",
    "    date = data.index\n",
    "\n",
    "    if windsorize:\n",
    "        for clm in data.columns:\n",
    "            max_p = np.percentile(data[clm], 99.9)\n",
    "            min_p = np.percentile(data[clm], 0.1)\n",
    "\n",
    "            data.loc[data[clm] > max_p, clm] = max_p\n",
    "            data.loc[data[clm] < min_p, clm] = min_p\n",
    "    \n",
    "    return data, date, namelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_day = 15\n",
    "back_day = list(range(back_day))\n",
    "window_length = 6*250\n",
    "train_size = 1000 #! MODIFIED for smaller dataset\n",
    "count_one_day = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_data = pd.read_csv('/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/data/processed-5yr-93-minute/65min_rv.csv', index_col=0)\n",
    "rv_data, date, namelist = initial_preprocess(rv_data, windsorize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self, input_dfs: List[pd.DataFrame], target, back_day: List, forward_day: int):\n",
    "        self.input_dfs = input_dfs\n",
    "        self.target = target\n",
    "        self.back_day = back_day\n",
    "        self.forward_day = forward_day\n",
    "        \n",
    "        self._generate_shifted_sequences()\n",
    "        self._generate_target_mask_and_date()\n",
    "\n",
    "    def _generate_shifted_sequences(self):\n",
    "        self.x = []\n",
    "        for df in self.input_dfs: # for each input df\n",
    "            shifted_df = pd.concat(\n",
    "                [df.shift(n) for n in self.back_day], axis=1 # shift and concatenate\n",
    "            ).reset_index(drop=True).iloc[:, ::-1] # reset index and reverse order\n",
    "\n",
    "            self.x.append(np.expand_dims(np.array(shifted_df), axis=2)) # expand dims and append on the new dim\n",
    "\n",
    "        self.x = np.concatenate(tuple(self.x), axis=2) # concatenate on the new dim (now we have a single 3d array)\n",
    "        # Sanity check\n",
    "        assert self.x.shape == (self.input_dfs[0].shape[0], len(self.back_day), len(self.input_dfs)), \"Input shape is incorrect\"\n",
    "\n",
    "    def _generate_target_mask_and_date(self):\n",
    "        non_na_mask = [~np.any(np.isnan(p)) for p in self.x] # make a mask for non-nan values over all features\n",
    "        \n",
    "        # print(f\"Target type: {type(self.target)}\") #? Debug\n",
    "        if type(self.target) == pd.Series: # if the target is a series\n",
    "            self.target = pd.DataFrame(self.target) # convert to df to avoid error in line below where we call .all() and axis=1\n",
    "        \n",
    "        self.y = self.target.shift(-self.forward_day).reset_index(drop=True) # shift target\n",
    "        valid_target_mask = self.y.notna().all(axis=1) # make a mask for valid target values\n",
    "        self.final_mask = np.logical_and(non_na_mask, valid_target_mask) # combine masks\n",
    "        \n",
    "        self.x = self.x[self.final_mask] # apply mask\n",
    "        self.y = np.array(self.y[self.final_mask].reset_index(drop=True)) # apply mask\n",
    "        self.idx = self.target.index[self.final_mask] # get all dates\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def train(self, x, y):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, x):\n",
    "        pass\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.coefficients = None\n",
    "        self.fit_intercept = fit_intercept  # Added\n",
    "\n",
    "    def train(self, X, y):\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"Mismatched dimensions: X has {} rows but y has {} rows\".format(X.shape[0], y.shape[0]))\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X = self._add_bias_term(X)  # Added\n",
    "\n",
    "        self.coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.coefficients is None:\n",
    "            raise ValueError(\"Model has not been trained yet.\")\n",
    "\n",
    "        if self.fit_intercept: \n",
    "            X = self._add_bias_term(X)  # Added\n",
    "\n",
    "        return X @ self.coefficients\n",
    "\n",
    "    def _add_bias_term(self, X):  # Added\n",
    "        return np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)  # Added\n",
    "\n",
    "class RollingPredict:\n",
    "    def __init__(self, back_day, data, namelist, window_length):\n",
    "        self.back_day = back_day\n",
    "        self.data = data # store data in attribute\n",
    "        self.namelist = namelist # store namelist in attribute\n",
    "        self.window_length = window_length\n",
    "\n",
    "        self._rolling_preprocess() # preprocess all the data fort his window\n",
    "\n",
    "        # assert everything is good\n",
    "        assert self.preprocess_obj.x.shape[0] == self.preprocess_obj.y.shape[0], \"X and Y shape mismatch\"\n",
    "        assert self.preprocess_obj.x.shape[0] == len(self.preprocess_obj.idx), \"X and IDX shape mismatch\"\n",
    "        assert self.preprocess_obj.x.shape[0] == len(self.preprocess_obj.idx), \"Y and IDX shape mismatch\"\n",
    "\n",
    "    def _rolling_preprocess(self):\n",
    "        initial_df = [self.data[self.namelist[0]+'_logvol']] # get the first df\n",
    "        \n",
    "        self.preprocess_obj = Preprocess(initial_df, self.data[self.namelist[0]+'_logvol'], self.back_day, self.window_length)\n",
    "\n",
    "        for ticker in self.namelist[1:]:\n",
    "            temp_df = [self.data[ticker + '_logvol']]\n",
    "            temp_preprocess_obj = Preprocess(temp_df, self.data[ticker + '_logvol'], self.back_day, self.window_length)\n",
    "\n",
    "            self._concatenate_preprocessed_data(temp_preprocess_obj)\n",
    "\n",
    "    def _concatenate_preprocessed_data(self, temp_preprocess_obj):\n",
    "        self.preprocess_obj.x = np.concatenate([self.preprocess_obj.x, temp_preprocess_obj.x], axis=0)\n",
    "        self.preprocess_obj.y = np.concatenate([self.preprocess_obj.y, temp_preprocess_obj.y], axis=0)\n",
    "        self.preprocess_obj.idx = np.concatenate([self.preprocess_obj.idx, temp_preprocess_obj.idx], axis=0)\n",
    "\n",
    "    def train(self, train_index, predict_index):\n",
    "        # Find the indices that correspond to the starting points of our training window.\n",
    "        temp_train_start = np.where(self.preprocess_obj.idx == train_index[0])[0]\n",
    "        \n",
    "        # Create a list of indices that correspond to all data points in the training window.\n",
    "        # For each starting point found, we create a range that defines our actual training window.\n",
    "        temp_index_train = [i for start in temp_train_start for i in range(start, start + len(train_index))]\n",
    "        \n",
    "        # same for predict\n",
    "        temp_predict_start = np.where(self.preprocess_obj.idx == predict_index[0])[0]\n",
    "        temp_index_predict = [i for start in temp_predict_start for i in range(start, start + len(predict_index))]\n",
    "        \n",
    "        train_x = self.preprocess_obj.x[temp_index_train]\n",
    "        train_y = self.preprocess_obj.y[temp_index_train]\n",
    "        test_x = self.preprocess_obj.x[temp_index_predict]\n",
    "        test_y = self.preprocess_obj.y[temp_index_predict]\n",
    "        \n",
    "        # reshape features\n",
    "        train_x = train_x.reshape(train_x.shape[0], -1)\n",
    "        test_x = test_x.reshape(test_x.shape[0], -1)\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.train(train_x, train_y)\n",
    "        \n",
    "        predictions = model.predict(test_x) \n",
    "        \n",
    "        # Reshape data for side-by-side comparison.\n",
    "        predictions = np.reshape(predictions, (-1, len(self.namelist)), 'F')\n",
    "        test_y = np.reshape(test_y, (-1, len(self.namelist)), 'F')\n",
    "\n",
    "        results_df = pd.DataFrame(np.concatenate((predictions, test_y), axis=1))\n",
    "        \n",
    "        # ! CONCISE VERSION\n",
    "        # results_df.index = self.preprocess_obj.idx[\n",
    "        #                    (np.where(self.preprocess_obj.idx == predict_index[0])[0][0] + 1):\n",
    "        #                    (np.where(self.preprocess_obj.idx == predict_index[-1])[0][0] + 2)]\n",
    "        # results_df.columns = [x + '_predicted' for x in self.namelist] + [x + '_actual' for x in self.namelist]\n",
    "\n",
    "        # ! VERSION FOR READABILITY \n",
    "        start_idx_position = np.where(self.preprocess_obj.idx == predict_index[0])[0][0] # find the index of the first predict index\n",
    "        end_idx_position = np.where(self.preprocess_obj.idx == predict_index[-1])[0][0] # find the index of the last predict index\n",
    "\n",
    "        # Adjust the starting and ending positions to match the DataFrame index.\n",
    "        adjusted_start = start_idx_position + 1\n",
    "        adjusted_end = end_idx_position + 2\n",
    "\n",
    "        results_df.index = self.preprocess_obj.idx[adjusted_start:adjusted_end]\n",
    "\n",
    "        predicted_columns = [x + '_predicted' for x in self.namelist]\n",
    "        actual_columns = [x + '_actual' for x in self.namelist]\n",
    "        results_df.columns = predicted_columns + actual_columns\n",
    "\n",
    "        # Step 13: Return the results DataFrame.\n",
    "        return results_df\n",
    "\n",
    "    def run(self, window_length, train_size, Epoch_num=0, lr=None):\n",
    "        # Calculate the total number of observations\n",
    "        T = int(self.preprocess_obj.x.shape[0] / len(self.namelist))\n",
    "        result_list = []\n",
    "\n",
    "        # Set the starting date for the test data\n",
    "        test_start_dt_str = '2020-06-30 10:35'\n",
    "        test_start_dt = pd.Timestamp(test_start_dt_str, tz='US/Eastern')\n",
    "        # TODO - make this a parameter or something\n",
    "\n",
    "        assert test_start_dt in self.preprocess_obj.idx, \"Test start date is not in the index\"\n",
    "        start_index = np.where(self.preprocess_obj.idx == test_start_dt)[0][0] \n",
    "\n",
    "        # Loop through the data in windows\n",
    "        for start in range(start_index, T - 1, window_length):\n",
    "            print(self.preprocess_obj.idx[start]) #? Debugging\n",
    "\n",
    "            # Determine the range for training and prediction\n",
    "            train_idx_start = self.preprocess_obj.idx[start_index - train_size:start]\n",
    "            \n",
    "            # min function to handle the edge case for the last window\n",
    "            predict_idx_range = self.preprocess_obj.idx[start:min(start + window_length, T - 1)]\n",
    "\n",
    "            kwargs = {\n",
    "                \"train_index\": train_idx_start,\n",
    "                \"predict_index\": predict_idx_range,\n",
    "            }\n",
    "\n",
    "            if Epoch_num != 0: kwargs[\"Epoch_num\"] = Epoch_num # for the NN\n",
    "            if lr != None: kwargs[\"lr\"] = lr # for the NN\n",
    "\n",
    "            result = self.train(**kwargs)\n",
    "            result_list.append(result)\n",
    "\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30 10:35:00-04:00\n",
      "2021-06-28 14:55:00-04:00\n",
      "2022-06-27 10:35:00-04:00\n"
     ]
    }
   ],
   "source": [
    "rp_args = [back_day, rv_data, namelist, window_length]\n",
    "q = RollingPredict(*rp_args)\n",
    "\n",
    "result = q.run(window_length, train_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
