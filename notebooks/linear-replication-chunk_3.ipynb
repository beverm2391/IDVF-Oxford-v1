{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/data/processed-5yr-93-minute/65min.csv\", index_col=0)\n",
    "num_agg_bars = data.shape[0] # number of aggregate bars\n",
    "# assuming 6 65-minute periods per day\n",
    "back_day = 6*20 # 20 days\n",
    "window_length = 6*250 # 250 days\n",
    "\n",
    "data.ffill(inplace=True)\n",
    "data.bfill(inplace=True)\n",
    "assert data.isna().sum().sum() == 0\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], utc=True).dt.tz_convert('US/Eastern')\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "namelist = data.columns.tolist()\n",
    "\n",
    "def rv(series: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Realized volatility is defined in [Volatility Forecasting with Machine Learning\n",
    "    and Intraday Commonality](https://arxiv.org/pdf/2202.08962.pdf) as:\n",
    "\n",
    "    $$RV_{i,t}(h)=\\log(\\sum_{s=t-h+1}^{t}r^2_{i,s})$$\n",
    "    \"\"\"\n",
    "    assert window > 0, \"Window must be greater than 0\"\n",
    "    fuzz = 1e-16\n",
    "    log_returns = np.log(series).diff() # log returns\n",
    "    sum_of_squares = log_returns.rolling(window=window).apply(lambda x: np.sum(x**2), raw=True)\n",
    "    rv = np.log(sum_of_squares + fuzz)\n",
    "    assert rv.isna().sum() == window, \"RV should have NaNs at the beginning\" # ? should have one nan from logret and window - 1 from rolling = window\n",
    "    return rv\n",
    "\n",
    "for ind in namelist:\n",
    "    data[ind + \"_logvol\"] = rv(data[ind], window_length)\n",
    "\n",
    "date = data.index\n",
    "\n",
    "assert data.shape == (num_agg_bars, len(namelist)*2), \"Dataframe shape is incorrect, should be the same number of rows as raw data but with double columns because we should have a price col and vol col for each ticker \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    # ! They put all the logic in the class constructor - i would prefer to have a separate function for each step\n",
    "    def __init__(self, input : List[pd.DataFrame], target, back_day : list = list(range(0,15)), forward_day = 1):\n",
    "        # this liss(range(0,15)), forward_day = 1 seem to correspond to the lookback window and the forecast horizon\n",
    "        # ! input is a list of dataframes, for example [price,volatility] with index as the same as target.\n",
    "        # ! note that the original code in rolling_predict passes one stock at a time, so the input is a list of one dataframe\n",
    "        # list of dfs holds all the results, target is the actual \"input\" column\n",
    "        \n",
    "        # ! Section 1 - make incrementally shifted seqences for each df in the input list\n",
    "        self.x = []\n",
    "        for df in input:\n",
    "            # Shift the dataframe by each value in back_day and concatenate along columns\n",
    "            # ! detailed explanation below\n",
    "            shifted_df = pd.concat(\n",
    "                list(map(lambda n: df.shift(n), back_day)), axis=1\n",
    "            ).reset_index(drop=True).loc[:, ::-1]\n",
    "            self.x.append(np.expand_dims(np.array(shifted_df), axis=2)) # Expand dimensions to make it compatible for future concatenation\n",
    "\n",
    "        self.x = np.concatenate(tuple(self.x), axis=2) # Concatenate all processed input data along the last axis to return a 3D array\n",
    "\n",
    "        # ! X shape = (7516, 15, 1), rows / columns / channels\n",
    "        # ! X shape = (number of agg bars) / (back day list len) / (#dfs in input list)\n",
    "        assert self.x.shape == (input[0].shape[0], len(back_day), len(input)), \"Input shape is incorrect\" # ! this is a sanity check to make sure the shape is correct\n",
    "        \n",
    "        # ! Section 2 - make the target, mask, and date\n",
    "        idx1 = [~np.any(np.isnan(p)) for p in self.x] # Create an index mask where none of the elements in the x dataframes are NaN\n",
    "        # ! for each row in x (which includes the row data from all dfs), if any of the values are NaN, then return False, else return True (therefore the length of idx will be the number of rows)\n",
    "        self.y = target.shift(-forward_day) # Shift the target by forward_day to align with predictor variables\n",
    "        self.y = pd.DataFrame(self.y).reset_index(drop=True) # Reset index to align with self.x (i removed the double parentheses around self.y)\n",
    "        self.idx2 = self.y.notna().all(axis=1) # simple mask all rows where there are no NaNs (i.e. all values are present) note that this is notna, not isna like before. So this is the opposite of the previous mask\n",
    "        self.idx = np.logical_and(idx1, self.idx2) # Combine the two index masks (element-wise and)\n",
    "        # ! final mask \"idx\" is of shape (rows, )\n",
    "\n",
    "        self.x = self.x[self.idx] # Filter x and y data based on combined index mask\n",
    "        self.y = np.array(self.y[self.idx].reset_index(drop=True)) # Filter date based on combined index mask, make it an array (this changes expected dims)\n",
    "        \n",
    "        # ! Section 3 - make the date\n",
    "        self.idx = data.index[self.idx] #! this is weird naming convention, because now self.idx is the date index from the data df, not a mask anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingPredict:\n",
    "    def __init__(self, keywords = ['XVZ_volatility'], back_day = list(range(0, 15)), lr=0.001):\n",
    "        self.back_day = back_day # list of days to look back\n",
    "        self.lr = lr # learning rate\n",
    "        self.keywords = keywords\n",
    "\n",
    "        first_df = [data[namelist[0]+'_logvol']] # temporary list of dataframes (only one element for now)\n",
    "        self.a = Preprocess(first_df, data[namelist[0]+'_logvol'], back_day = back_day) # preprocess the data\n",
    "        # ! self.a is an object with attributes x, y, and idx\n",
    "        # self.a is a preprocess object\n",
    "        # ! nans = number of original agg bars - window_length - back_day list length\n",
    "            # There might be an edge case where the log_vol window length is longer than the window length, which could mess this up\n",
    "        # ! preprocessed agg bars (ppd agg bars) = number of original agg bars - window_length - back_day list length\n",
    "        # self.a.x is a numpy array of shape (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "        # self.a.y is a numpy array of shape (6001, 1) / (ppd agg bars - nans, 1)\n",
    "        # self.a.idx is a pandas index of timestamps of shape (6001, ) / (ppd agg bars, )\n",
    "\n",
    "        for ind in namelist[1:]: # iterate over each ticker\n",
    "            temp = [data[ind + '_logvol']] # make a list of dataframes (only one element for now) (this is passed to the input attribute of Preprocess)\n",
    "            # Shape of temp = (7516, 1)\n",
    "            temp_a = Preprocess(temp, data[ind + '_logvol'], back_day = back_day) \n",
    "\n",
    "            # ! Expected Dims for temp_a\n",
    "            # temp_a.x = (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "\n",
    "            # ! these lines essentially concat onto the first_df object stored in self.a\n",
    "            # ! So theyre updating the x, y, and idx attributes of self.a\n",
    "            # ! not sure why they did this\n",
    "            self.a.x = np.concatenate([self.a.x, temp_a.x], axis=0) # concatenate the x data (predictor variables)\n",
    "            # now (first loop) self.a.x should be of shape (12002, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "            self.a.y = np.concatenate([self.a.y, temp_a.y], axis=0) # concatenate the y data (target variable)\n",
    "            # now (first loop) self.a.y should be of shape (12002, 1) / (ppd agg bars, 1)\n",
    "            self.a.idx = np.concatenate([self.a.idx, temp_a.idx], axis=0) # concatenate the date data\n",
    "            # now (first loop) self.a.idx should be of shape (12002, ) / (ppd agg bars, )\n",
    "\n",
    "        ppd_agg_bars = first_df[0].shape[0] - window_length - len(back_day) # number of agg bars - window length - back day list length\n",
    "        all_ppd_agg_bars = ppd_agg_bars * len(namelist) # total number of agg bars * number of tickers\n",
    "        assert self.a.x.shape == (all_ppd_agg_bars, len(back_day), 1), \"Preprocessed df should have dimensions (ppd agg bars * name list len, len back day list, 1)\"\n",
    "        assert self.a.y.shape == (all_ppd_agg_bars, 1), \"Preprocessed df should have dimensions (ppd agg bars, 1)\"\n",
    "        assert self.a.idx.shape == (all_ppd_agg_bars, ), \"Preprocessed df should have dimensions (ppd agg bars, )\"\n",
    "\n",
    "    def train(self, train_index, predict_index, lr, names, Epoch_num = 300, pre=True):\n",
    "        assert len(train_index) > 0, \"Train index must not be empty\"\n",
    "        assert len(predict_index) > 0, \"Predict index must not be empty\"\n",
    "\n",
    "        # ! Need to figure out how all this works ========================\n",
    "        temp_train_start = np.where(self.a.idx == train_index[0]) # match the date index stored in self.a.idx to the first training date\n",
    "        # TODO - figure out the data structure of temp_train_start\n",
    "        temp_index_train = [] # list of indices for training data\n",
    "\n",
    "        for i in temp_train_start[0]: # for each index in temp_train_start[0]\n",
    "            temp_index_train.extend(list(range(i, i + len(train_index)))) # add the indices for the training data\n",
    "\n",
    "        temp_predict_start = np.where(self.a.idx == predict_index[0]) # get the index of the first prediction date\n",
    "        temp_index_predict = []\n",
    "\n",
    "        for i in temp_predict_start[0]:\n",
    "            temp_index_predict.extend(list(range(i, i + len(predict_index)))) # add the indices for the prediction data\n",
    "        \n",
    "        train_x = self.a.x[temp_index_train] # get the training predictor data\n",
    "        train_y = self.a.y[temp_index_train] # get the training target data\n",
    "        test_x = self.a.x[temp_index_predict] # get the test predictor data\n",
    "        test_y = self.a.y[temp_index_predict] # get the test target data\n",
    "\n",
    "        train_x = train_x.reshape(train_x.shape[0], -1) # reshape the training predictor data\n",
    "        test_x = test_x.reshape(test_x.shape[0], -1) # reshape the test predictor data\n",
    "\n",
    "        # ! bias term\n",
    "        train_x = np.concatenate((np.ones((train_x.shape[0], 1)), train_x), axis=1) # add a column of ones to the training predictor data\n",
    "        test_x = np.concatenate((np.ones((test_x.shape[0], 1)), test_x), axis=1) # add a column of ones to the test predictor data\n",
    "\n",
    "        # ! get beta\n",
    "        def _get_beta(x, y):\n",
    "            \"\"\"\n",
    "            Get the coefficients of the linear regression model\n",
    "            $$\\beta = \\left( X^T X \\right)^{-1} X^T y$$\n",
    "            \"\"\"\n",
    "            return np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T), y)\n",
    "        \n",
    "        beta = _get_beta(train_x, train_y) # get the beta coefficients\n",
    "        pred = np.matmul(test_x, beta) # get the predictions\n",
    "\n",
    "        pred = np.reshape(pred, (-1, len(namelist)), 'F') # reshape the predictions, fortan style\n",
    "        test_y = np.reshape(test_y, (-1, len(namelist)), 'F')\n",
    "\n",
    "        plot_valid = np.concatenate((pred, test_y), axis=1)\n",
    "        plot_valid = pd.DataFrame(plot_valid)\n",
    "        # Locate the indices in the 'date' array where the prediction interval starts and ends\n",
    "        start_date_index = np.where(date == predict_index[0])[0][0]\n",
    "        end_date_index = np.where(date == predict_index[-1])[0][0]\n",
    "        start_date_index += 1 # Increment indices to align with the desired date range\n",
    "        end_date_index += 2\n",
    "        plot_valid.index = date[start_date_index:end_date_index] # Create the new date range for the index of 'plot_valid'\n",
    "\n",
    "        plot_valid.columns = [x + 'out' for x in namelist] + [x + 'real' for x in namelist]\n",
    "        return plot_valid # returns the prediction and the real value\n",
    "\n",
    "        # ! ===============================================================\n",
    "        \n",
    "    def run(self, window_length, train_size=None, Epoch_num = 2, pre=True):\n",
    "        # from the paper:\n",
    "        # Our testing period starts from July 1, 2015 until June 30, 2016, and the corresponding training and validation samples are [July 1, 2011, June 30, 2014] and [July 1, 2014, June 30, 2015],\n",
    "        test_start_date = '2020-06-30' # this is the last date of train, so test will start on the next day\n",
    "        test_start_date = pd.Timestamp(f'{test_start_date} 9:30', tz='US/Eastern')\n",
    "\n",
    "        seq_per_stock = int(self.a.x.shape[0]/len(namelist)) # use int to prevent float. T is the number of agg bars per ticker, so ppd agg bars\n",
    "        result_list = []\n",
    "        start_index = np.where(self.a.idx == test_start_date)[0][0] # get the index of the first prediction date for the first ticker\n",
    "\n",
    "        if train_size is None:\n",
    "            train_size = start_index # if train size is not specified, then set it to the start index\n",
    "        if train_size > start_index:\n",
    "            raise ValueError(f\"Train size must be less than the start index. Your train size {train_size} is greater than the start index {start_index}\")\n",
    "\n",
    "        range_len = (seq_per_stock - start_index) // window_length + 1 # Example (expected number of windows)\n",
    "        for window_start in range(start_index,  seq_per_stock-1, window_length): # creates a range with (start_idx, ppd_aggs -1, window_length) (i, o, step)\n",
    "            # print(self.a.idx[window_start])\n",
    "\n",
    "            window_end = window_start + window_length\n",
    "            training_start = start_index - train_size\n",
    "            assert training_start >= 0, f\"Training start must be greater than 0. Your training size {train_size} is greater than window size {window_length}\"\n",
    "            training_end = window_start # this will increment by window_length each iteration, increasing the training set size by window_length each time\n",
    "\n",
    "            # ! The if/else logic prevents an index out-of-bounds error that could occur during the last iteration of the rolling window approach, especially when the remaining data points at the end of the dataset are fewer than the specified window_length\n",
    "            if window_end <= seq_per_stock - 1: # if the end of the window is less than the end of the test set\n",
    "                train_indices = self.a.idx[training_start:training_end]\n",
    "                predict_indices = self.a.idx[window_start:window_end]\n",
    "            else:\n",
    "                train_indices = self.a.idx[training_start:training_end]\n",
    "                predict_indices = self.a.idx[window_start:seq_per_stock - 1]\n",
    "\n",
    "            result = self.train(\n",
    "                Epoch_num=Epoch_num,\n",
    "                train_index=train_indices,\n",
    "                predict_index=predict_indices,\n",
    "                lr=None,\n",
    "                names=None,\n",
    "                pre=pre\n",
    "            )\n",
    "            result_list.append(result)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = RollingPredict(back_day= list(range(0, 15)), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = q.run(window_length, Epoch_num = 20000, pre = False) # ! I dont think this epoch number is changing anything, fix\n",
    "result = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(result: pd.DataFrame):\n",
    "    report_df = pd.DataFrame(index = namelist,columns=['MSE','r2_score']) # init report df with MSE and r square\n",
    "    for i in namelist:\n",
    "        report_df.loc[i,'MSE'] = mean_squared_error(result[i+'out'],result[i+'real']) # calculate MSE\n",
    "        report_df.loc[i,'r2_score'] = r2_score( result[i + 'real'],result[i + 'out']) # calculate r square\n",
    "        report_df.loc[i,'MAPE'] = mean_absolute_percentage_error( result[i + 'real'],result[i + 'out']) # calculate MAPE\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MSFT cols\n",
    "msft_cols = [col for col in result.columns if 'MSFT' in col]\n",
    "msft_result = result[msft_cols].copy() # copy to prevent SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_true): return np.mean((y - y_true)**2)\n",
    "def MAPE(y, y_true): return np.mean(np.abs((y - y_true) / y_true)) * 100\n",
    "\n",
    "msft_result['MSE'] = msft_result.apply(lambda x: MSE(x['MSFTout'], x['MSFTreal']), axis=1)\n",
    "msft_result['MAPE'] = msft_result.apply(lambda x: MAPE(x['MSFTout'], x['MSFTreal']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = report(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0282852565417069e-05"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_df.loc['MSFT','MSE']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
