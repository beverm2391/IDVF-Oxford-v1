{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from typing import List\n",
    "import os\n",
    "from scipy.stats import mstats\n",
    "\n",
    "from lib.utils import get_nyse_calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/data/processed-5yr-93-minute/65min.csv\", index_col=0)\n",
    "num_agg_bars = data.shape[0] # number of aggregate bars\n",
    "# assuming 6 65-minute periods per day\n",
    "back_day = 6*20 # 20 days\n",
    "window_length = 6*250 # 250 days\n",
    "train_size = 6*1000 # 1000 days\n",
    "\n",
    "data.ffill(inplace=True)\n",
    "data.bfill(inplace=True)\n",
    "assert data.isna().sum().sum() == 0\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], utc=True).dt.tz_convert('US/Eastern')\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "namelist = data.columns.tolist()\n",
    "\n",
    "def rv(series: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Realized volatility is defined in [Volatility Forecasting with Machine Learning\n",
    "    and Intraday Commonality](https://arxiv.org/pdf/2202.08962.pdf) as:\n",
    "\n",
    "    $$RV_{i,t}(h)=\\log(\\sum_{s=t-h+1}^{t}r^2_{i,s})$$\n",
    "    \"\"\"\n",
    "    assert window > 0, \"Window must be greater than 0\"\n",
    "    fuzz = 1e-16\n",
    "    log_returns = np.log(series).diff() # log returns\n",
    "    sum_of_squares = log_returns.rolling(window=window).apply(lambda x: np.sum(x**2), raw=True)\n",
    "    rv = np.log(sum_of_squares + fuzz)\n",
    "    assert rv.isna().sum() == window, \"RV should have NaNs at the beginning\" # ? should have one nan from logret and window - 1 from rolling = window\n",
    "    return rv\n",
    "\n",
    "for ind in namelist:\n",
    "    data[ind + \"_logvol\"] = rv(data[ind], window_length)\n",
    "\n",
    "date = data.index\n",
    "\n",
    "assert data.shape == (num_agg_bars, len(namelist)*2), \"Dataframe shape is incorrect, should be the same number of rows as raw data but with double columns because we should have a price col and vol col for each ticker \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    # ! They put all the logic in the class constructor - i would prefer to have a separate function for each step\n",
    "    def __init__(self, input : List[pd.DataFrame], target, back_day : list = list(range(0,15)), forward_day = 1):\n",
    "        # this liss(range(0,15)), forward_day = 1 seem to correspond to the lookback window and the forecast horizon\n",
    "        # ! input is a list of dataframes, for example [price,volatility] with index as the same as target.\n",
    "        # ! note that the original code in rolling_predict passes one stock at a time, so the input is a list of one dataframe\n",
    "        # list of dfs holds all the results, target is the actual \"input\" column\n",
    "        \n",
    "        # ! Section 1 - make incrementally shifted seqences for each df in the input list\n",
    "        self.x = []\n",
    "        for df in input:\n",
    "            # Shift the dataframe by each value in back_day and concatenate along columns\n",
    "            # ! detailed explanation below\n",
    "            shifted_df = pd.concat(\n",
    "                list(map(lambda n: df.shift(n), back_day)), axis=1\n",
    "            ).reset_index(drop=True).loc[:, ::-1]\n",
    "            self.x.append(np.expand_dims(np.array(shifted_df), axis=2)) # Expand dimensions to make it compatible for future concatenation\n",
    "\n",
    "        self.x = np.concatenate(tuple(self.x), axis=2) # Concatenate all processed input data along the last axis to return a 3D array\n",
    "\n",
    "        # ! X shape = (7516, 15, 1), rows / columns / channels\n",
    "        # ! X shape = (number of agg bars) / (back day list len) / (#dfs in input list)\n",
    "        assert self.x.shape == (input[0].shape[0], len(back_day), len(input)), \"Input shape is incorrect\" # ! this is a sanity check to make sure the shape is correct\n",
    "        \n",
    "        # ! Section 2 - make the target, mask, and date\n",
    "        idx1 = [~np.any(np.isnan(p)) for p in self.x] # Create an index mask where none of the elements in the x dataframes are NaN\n",
    "        # ! for each row in x (which includes the row data from all dfs), if any of the values are NaN, then return False, else return True (therefore the length of idx will be the number of rows)\n",
    "        self.y = target.shift(-forward_day) # Shift the target by forward_day to align with predictor variables\n",
    "        self.y = pd.DataFrame(self.y).reset_index(drop=True) # Reset index to align with self.x (i removed the double parentheses around self.y)\n",
    "        self.idx2 = self.y.notna().all(axis=1) # simple mask all rows where there are no NaNs (i.e. all values are present) note that this is notna, not isna like before. So this is the opposite of the previous mask\n",
    "        self.idx = np.logical_and(idx1, self.idx2) # Combine the two index masks (element-wise and)\n",
    "        # ! final mask \"idx\" is of shape (rows, )\n",
    "\n",
    "        self.x = self.x[self.idx] # Filter x and y data based on combined index mask\n",
    "        self.y = np.array(self.y[self.idx].reset_index(drop=True)) # Filter date based on combined index mask, make it an array (this changes expected dims)\n",
    "        \n",
    "        # ! Section 3 - make the date\n",
    "        self.idx = data.index[self.idx] #! this is weird naming convention, because now self.idx is the date index from the data df, not a mask anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingPredict:\n",
    "    def __init__(self, keywords = ['XVZ_volatility'], back_day = list(range(0, 15)), lr=0.001):\n",
    "        self.back_day = back_day # list of days to look back\n",
    "        self.lr = lr # learning rate\n",
    "        self.keywords = keywords\n",
    "\n",
    "        first_df = [data[namelist[0]+'_logvol']] # temporary list of dataframes (only one element for now)\n",
    "        self.a = Preprocess(first_df, data[namelist[0]+'_logvol'], back_day = back_day) # preprocess the data\n",
    "        # ! self.a is an object with attributes x, y, and idx\n",
    "        # self.a is a preprocess object\n",
    "        # ! nans = number of original agg bars - window_length - back_day list length\n",
    "            # There might be an edge case where the log_vol window length is longer than the window length, which could mess this up\n",
    "        # ! preprocessed agg bars (ppd agg bars) = number of original agg bars - window_length - back_day list length\n",
    "        # self.a.x is a numpy array of shape (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "        # self.a.y is a numpy array of shape (6001, 1) / (ppd agg bars - nans, 1)\n",
    "        # self.a.idx is a pandas index of timestamps of shape (6001, ) / (ppd agg bars, )\n",
    "\n",
    "        for ind in namelist[1:]: # iterate over each ticker\n",
    "            temp = [data[ind + '_logvol']] # make a list of dataframes (only one element for now) (this is passed to the input attribute of Preprocess)\n",
    "            # Shape of temp = (7516, 1)\n",
    "            temp_a = Preprocess(temp, data[ind + '_logvol'], back_day = back_day) \n",
    "\n",
    "            # ! Expected Dims for temp_a\n",
    "            # temp_a.x = (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "\n",
    "            # ! these lines essentially concat onto the first_df object stored in self.a\n",
    "            # ! So theyre updating the x, y, and idx attributes of self.a\n",
    "            # ! not sure why they did this\n",
    "            self.a.x = np.concatenate([self.a.x, temp_a.x], axis=0) # concatenate the x data (predictor variables)\n",
    "            # now (first loop) self.a.x should be of shape (12002, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "            self.a.y = np.concatenate([self.a.y, temp_a.y], axis=0) # concatenate the y data (target variable)\n",
    "            # now (first loop) self.a.y should be of shape (12002, 1) / (ppd agg bars, 1)\n",
    "            self.a.idx = np.concatenate([self.a.idx, temp_a.idx], axis=0) # concatenate the date data\n",
    "            # now (first loop) self.a.idx should be of shape (12002, ) / (ppd agg bars, )\n",
    "\n",
    "        ppd_agg_bars = first_df[0].shape[0] - window_length - len(back_day) # number of agg bars - window length - back day list length\n",
    "        all_ppd_agg_bars = ppd_agg_bars * len(namelist) # total number of agg bars * number of tickers\n",
    "        assert self.a.x.shape == (all_ppd_agg_bars, len(back_day), 1), \"Preprocessed df should have dimensions (ppd agg bars * name list len, len back day list, 1)\"\n",
    "        assert self.a.y.shape == (all_ppd_agg_bars, 1), \"Preprocessed df should have dimensions (ppd agg bars, 1)\"\n",
    "        assert self.a.idx.shape == (all_ppd_agg_bars, ), \"Preprocessed df should have dimensions (ppd agg bars, )\"\n",
    "\n",
    "    def train(self, train_index, predict_index, lr, names, Epoch_num = 300, pre=True):\n",
    "        print(\"Train Index ============================\")\n",
    "        print(train_index)\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        print(\"Predict Index ============================\")\n",
    "        print(predict_index)\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        print(\"A Idx ============================\")\n",
    "        print(self.a.idx)\n",
    "\n",
    "        # ! Need to figure out how all this works ========================\n",
    "        temp_train_start = np.where(self.a.idx == train_index[0]) # match the date index stored in self.a.idx to the first training date\n",
    "        # TODO - figure out the data structure of temp_train_start\n",
    "        temp_index_train = [] # list of indices for training data\n",
    "\n",
    "        for i in temp_train_start[0]: # for each index in temp_train_start[0]\n",
    "            temp_index_train.extend(list(range(i, i + len(train_index)))) # add the indices for the training data\n",
    "\n",
    "        temp_predict_start = np.where(self.a.idx == predict_index[0]) # get the index of the first prediction date\n",
    "        temp_index_predict = []\n",
    "\n",
    "        for i in temp_predict_start[0]:\n",
    "            temp_index_predict.extend(list(range(i, i + len(predict_index)))) # add the indices for the prediction data\n",
    "        \n",
    "        train_x = self.a.x[temp_index_train] # get the training predictor data\n",
    "        train_y = self.a.y[temp_index_train] # get the training target data\n",
    "        test_x = self.a.x[temp_index_predict] # get the test predictor data\n",
    "        test_y = self.a.y[temp_index_predict] # get the test target data\n",
    "\n",
    "        train_x = train_x.reshape(train_x.shape[0], -1) # reshape the training predictor data\n",
    "        test_x = test_x.reshape(test_x.shape[0], -1) # reshape the test predictor data\n",
    "\n",
    "        # ! bias term\n",
    "        train_x = np.concatenate((np.ones((train_x.shape[0], 1)), train_x), axis=1) # add a column of ones to the training predictor data\n",
    "        test_x = np.concatenate((np.ones((test_x.shape[0], 1)), test_x), axis=1) # add a column of ones to the test predictor data\n",
    "\n",
    "        # ! get beta\n",
    "        def _get_beta(x, y):\n",
    "            \"\"\"\n",
    "            Get the coefficients of the linear regression model\n",
    "            $$\\beta = \\left( X^T X \\right)^{-1} X^T y$$\n",
    "            \"\"\"\n",
    "            return np.matmul(np.matmul(np.linalg.inv(np.matmul(x.T, x)), x.T), y)\n",
    "        \n",
    "        beta = _get_beta(train_x, train_y) # get the beta coefficients\n",
    "        pred = np.matmul(test_x, beta) # get the predictions\n",
    "\n",
    "        pred = np.reshape(pred, (-1, len(namelist)), 'F') # reshape the predictions, fortan style\n",
    "        test_y = np.reshape(test_y, (-1, len(namelist)), 'F')\n",
    "\n",
    "        plot_valid = np.concatenate((pred, test_y), axis=1)\n",
    "        plot_valid = pd.DataFrame(plot_valid)\n",
    "        # Locate the indices in the 'date' array where the prediction interval starts and ends\n",
    "        start_date_index = np.where(date == predict_index[0])[0][0]\n",
    "        end_date_index = np.where(date == predict_index[-1])[0][0]\n",
    "        start_date_index += 1 # Increment indices to align with the desired date range\n",
    "        end_date_index += 2\n",
    "        plot_valid.index = date[start_date_index:end_date_index] # Create the new date range for the index of 'plot_valid'\n",
    "\n",
    "        plot_valid.columns = [x + 'out' for x in namelist] + [x + 'real' for x in namelist]\n",
    "        return plot_valid # returns the prediction and the real value\n",
    "\n",
    "        # ! ===============================================================\n",
    "        \n",
    "    def run(self, window_length, train_size, Epoch_num = 2, pre=True):\n",
    "        # from the paper:\n",
    "        # Our testing period starts from July 1, 2015 until June 30, 2016, and the corresponding training and validation samples are [July 1, 2011, June 30, 2014] and [July 1, 2014, June 30, 2015],\n",
    "        test_start_date = '2020-06-30' # this is the last date of train, so test will start on the next day\n",
    "        test_start_date = pd.Timestamp(f'{test_start_date} 9:30', tz='US/Eastern')\n",
    "\n",
    "        seq_per_stock = int(self.a.x.shape[0]/len(namelist)) # use int to prevent float. T is the number of agg bars per ticker, so ppd agg bars\n",
    "        result_list = []\n",
    "        start_index = np.where(self.a.idx == test_start_date)[0][0] # get the index of the first prediction date for the first ticker\n",
    "\n",
    "        for window_start in range(start_index,  seq_per_stock-1, window_length): # creates a range with (start_idx, ppd_aggs -1, window_length) (i, o, step)\n",
    "            print(self.a.idx[window_start])\n",
    "\n",
    "            window_end = window_start + window_length\n",
    "            training_start = start_index - train_size\n",
    "            training_end = window_start\n",
    "\n",
    "            # ! The if/else logic prevents an index out-of-bounds error that could occur during the last iteration of the rolling window approach, especially when the remaining data points at the end of the dataset are fewer than the specified window_length\n",
    "            if window_end <= seq_per_stock - 1: # if the end of the window is less than the end of the test set\n",
    "                train_indices = self.a.idx[training_start:training_end]\n",
    "                predict_indices = self.a.idx[window_start:window_end]\n",
    "            else:\n",
    "                train_indices = self.a.idx[training_start:training_end]\n",
    "                predict_indices = self.a.idx[window_start:seq_per_stock - 1]\n",
    "\n",
    "            result = self.train(\n",
    "                Epoch_num=Epoch_num,\n",
    "                train_index=train_indices,\n",
    "                predict_index=predict_indices,\n",
    "                lr=None,\n",
    "                names=None,\n",
    "                pre=pre\n",
    "            )\n",
    "            result_list.append(result)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = RollingPredict( back_day= list(range(0, 15)), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-30 09:30:00-04:00\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39;49mrun(window_length, train_size, Epoch_num \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m, pre \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m         train_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39midx[training_start:training_end]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m         predict_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39midx[window_start:seq_per_stock \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         Epoch_num\u001b[39m=\u001b[39;49mEpoch_num,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m         train_index\u001b[39m=\u001b[39;49mtrain_indices,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m         predict_index\u001b[39m=\u001b[39;49mpredict_indices,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m         lr\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m         names\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m         pre\u001b[39m=\u001b[39;49mpre\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     result_list\u001b[39m.\u001b[39mappend(result)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result_list\n",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_index, predict_index, lr, names, Epoch_num \u001b[39m=\u001b[39m \u001b[39m300\u001b[39m, pre\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     \u001b[39m# ! Need to figure out how all this works ========================\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     temp_train_start \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39midx \u001b[39m==\u001b[39m train_index[\u001b[39m0\u001b[39;49m]) \u001b[39m# match the date index stored in self.a.idx to the first training date\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# TODO - figure out the data structure of temp_train_start\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_3.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     temp_index_train \u001b[39m=\u001b[39m [] \u001b[39m# list of indices for training data\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "result = q.run(window_length, train_size, Epoch_num = 200, pre = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
