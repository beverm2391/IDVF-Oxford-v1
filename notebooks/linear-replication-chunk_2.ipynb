{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import mstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/data/processed-5yr-93-minute/65min.csv\", index_col=0)\n",
    "raw_data_shape = data.shape\n",
    "# assuming 6 65-minute periods per day\n",
    "back_day = 6*20 # 20 days\n",
    "window_length = 6*250 # 250 days\n",
    "train_size = 6*1000 # 1000 days\n",
    "\n",
    "data.ffill(inplace=True)\n",
    "data.bfill(inplace=True)\n",
    "assert data.isna().sum().sum() == 0\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "namelist = data.columns.tolist()\n",
    "\n",
    "def rv(series: pd.Series, window: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Realized volatility is defined in [Volatility Forecasting with Machine Learning\n",
    "    and Intraday Commonality](https://arxiv.org/pdf/2202.08962.pdf) as:\n",
    "\n",
    "    $$RV_{i,t}(h)=\\log(\\sum_{s=t-h+1}^{t}r^2_{i,s})$$\n",
    "    \"\"\"\n",
    "    assert window > 0, \"Window must be greater than 0\"\n",
    "    fuzz = 1e-16\n",
    "    log_returns = np.log(series).diff() # log returns\n",
    "    sum_of_squares = log_returns.rolling(window=window).apply(lambda x: np.sum(x**2), raw=True)\n",
    "    rv = np.log(sum_of_squares + fuzz)\n",
    "    assert rv.isna().sum() == window, \"RV should have NaNs at the beginning\" # ? should have one nan from logret and window - 1 from rolling = window\n",
    "    return rv\n",
    "\n",
    "for ind in namelist:\n",
    "    data[ind + \"_logvol\"] = rv(data[ind], window_length)\n",
    "\n",
    "date = data.index\n",
    "\n",
    "assert data.shape == (raw_data_shape[0], raw_data_shape[1]*2), \"Dataframe shape is incorrect, should be the same number of rows as raw data but with double columns because we should have a price col and vol col for each ticker \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    # ! They put all the logic in the class constructor - i would prefer to have a separate function for each step\n",
    "    def __init__(self, input, target, back_day = list(range(0,15)), forward_day = 1):\n",
    "        # this liss(range(0,15)), forward_day = 1 seem to correspond to the lookback window and the forecast horizon\n",
    "        # ! input is a list of dataframes, for example [price,volatility] with index as the same as target.\n",
    "        # ! note that the original code in rolling_predict passes one stock at a time, so the input is a list of one dataframe\n",
    "        # list of dfs holds all the results, target is the actual \"input\" column\n",
    "        \n",
    "        # ! Section 1 - make incrementally shifted seqences for each df in the input list\n",
    "        self.x = []\n",
    "        for df in input:\n",
    "            # Shift the dataframe by each value in back_day and concatenate along columns\n",
    "            # ! detailed explanation below\n",
    "            shifted_df = pd.concat(\n",
    "                list(map(lambda n: df.shift(n), back_day)), axis=1\n",
    "            ).reset_index(drop=True).loc[:, ::-1]\n",
    "            self.x.append(np.expand_dims(np.array(shifted_df), axis=2)) # Expand dimensions to make it compatible for future concatenation\n",
    "\n",
    "        self.x = np.concatenate(tuple(self.x), axis=2) # Concatenate all processed input data along the last axis to return a 3D array\n",
    "\n",
    "        # ! X shape = (7516, 15, 1), rows / columns / channels\n",
    "        # ! X shape = (number of agg bars) / (back day list len) / (#dfs in input list)\n",
    "        assert self.x.shape == (self.input[0].shape[0], len(back_day), len(input)), \"Input shape is incorrect\" # ! this is a sanity check to make sure the shape is correct\n",
    "        \n",
    "        # ! Section 2 - make the target, mask, and date\n",
    "        idx1 = [~np.any(np.isnan(p)) for p in self.x] # Create an index mask where none of the elements in the x dataframes are NaN\n",
    "        # ! for each row in x (which includes the row data from all dfs), if any of the values are NaN, then return False, else return True (therefore the length of idx will be the number of rows)\n",
    "        self.y = target.shift(-forward_day) # Shift the target by forward_day to align with predictor variables\n",
    "        self.y = pd.DataFrame(self.y).reset_index(drop=True) # Reset index to align with self.x (i removed the double parentheses around self.y)\n",
    "        self.idx2 = self.y.notna().all(axis=1) # simple mask all rows where there are no NaNs (i.e. all values are present) note that this is notna, not isna like before. So this is the opposite of the previous mask\n",
    "        self.idx = np.logical_and(idx1, self.idx2) # Combine the two index masks (element-wise and)\n",
    "        # ! final mask \"idx\" is of shape (rows, )\n",
    "\n",
    "        self.x = self.x[self.idx] # Filter x and y data based on combined index mask\n",
    "        self.y = np.array(self.y[self.idx].reset_index(drop=True)) # Filter date based on combined index mask, make it an array (this changes expected dims)\n",
    "        \n",
    "        # ! Section 3 - make the date\n",
    "        self.idx = data.index[self.idx] #! this is weird naming convention, because now self.idx is the date index from the data df, not a mask anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df = [data[namelist[0]+'_logvol']] # temporary list of dataframes (only one element for now)\n",
    "ppd = Preprocess(first_df, data[namelist[0]+'_logvol'], list(range(0, 15))) # preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_V2(x, levels=[0.01, 0.01]):\n",
    "    \"\"\"New windsorize function that works with 2D and 3D arrays, updated for readability\"\"\"\n",
    "    # Define a function to winsorize a 1D array\n",
    "    def winsorize_1d(arr):\n",
    "        return mstats.winsorize(arr, levels)\n",
    "\n",
    "    # Determine the axis along which to apply the winsorize function\n",
    "    # - For 2D arrays, apply along axis 0 (columns)\n",
    "    # - For 3D arrays, apply along axis 1 (rows within each 2D slice)\n",
    "    axis = 1 if len(x.shape) == 3 else 0\n",
    "\n",
    "    # Apply the winsorize function along the specified axis\n",
    "    y = np.apply_along_axis(winsorize_1d, axis, x)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RollingPredict:\n",
    "    def __init__(self, keywords = ['XVZ_volatility'], back_day = list(range(0, 15)), lr=0.001):\n",
    "        self.back_day = back_day # list of days to look back\n",
    "        self.lr = lr # learning rate\n",
    "        self.keywords = keywords\n",
    "\n",
    "        first_df = [data[namelist[0]+'_logvol']] # temporary list of dataframes (only one element for now)\n",
    "        self.a = Preprocess(first_df, data[namelist[0]+'_logvol'], back_day = back_day) # preprocess the data\n",
    "        # ! self.a is an object with attributes x, y, and idx\n",
    "        # self.a is a preprocess object\n",
    "        # ! nans = number of original agg bars - window_length - back_day list length\n",
    "        # ! preprocessed agg bars (ppd agg bars) = number of original agg bars - window_length - back_day list length\n",
    "        # self.a.x is a numpy array of shape (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "        # self.a.y is a numpy array of shape (6001, 1) / (ppd agg bars - nans, 1)\n",
    "        # self.a.idx is a pandas index of timestamps of shape (6001, ) / (ppd agg bars, )\n",
    "\n",
    "        for ind in namelist[1:]: # iterate over each ticker\n",
    "            temp = [data[ind + '_logvol']] # make a list of dataframes (only one element for now) (this is passed to the input attribute of Preprocess)\n",
    "            # Shape of temp = (7516, 1)\n",
    "            temp_a = Preprocess(temp, data[ind + '_logvol'], back_day = back_day) \n",
    "\n",
    "            # ! Expected Dims for temp_a\n",
    "            # temp_a.x = (6001, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "\n",
    "            # ! these lines essentially concat onto the first_df object stored in self.a\n",
    "            # ! So theyre updating the x, y, and idx attributes of self.a\n",
    "            # ! not sure why they did this\n",
    "            self.a.x = np.concatenate([self.a.x, temp_a.x], axis=0) # concatenate the x data (predictor variables)\n",
    "            # now (first loop) self.a.x should be of shape (12002, 15, 1) / (ppd agg bars, back day list len, #dfs in input list)\n",
    "            self.a.y = np.concatenate([self.a.y, temp_a.y], axis=0) # concatenate the y data (target variable)\n",
    "            # now (first loop) self.a.y should be of shape (12002, 1) / (ppd agg bars, 1)\n",
    "            self.a.idx = np.concatenate([self.a.idx, temp_a.idx], axis=0) # concatenate the date data\n",
    "            # now (first loop) self.a.idx should be of shape (12002, ) / (ppd agg bars, )\n",
    "\n",
    "        ppd_agg_bars = first_df[0].shape[0] - window_length - len(back_day) # number of agg bars - window length - back day list length\n",
    "        all_ppd_agg_bars = ppd_agg_bars * len(namelist) # total number of agg bars * number of tickers\n",
    "        assert self.a.x.shape == (all_ppd_agg_bars, len(back_day), 1), \"Preprocessed df should have dimensions (ppd agg bars * name list len, len back day list, 1)\"\n",
    "        assert self.a.y.shape == (all_ppd_agg_bars, 1), \"Preprocessed df should have dimensions (ppd agg bars, 1)\"\n",
    "        assert self.a.idx.shape == (all_ppd_agg_bars, ), \"Preprocessed df should have dimensions (ppd agg bars, )\"\n",
    "\n",
    "    def train(self, train_index, predict_index, lr, names, Epoch_num = 300, pre=True):\n",
    "\n",
    "        # ! Need to figure out how all this works ========================\n",
    "        temp_train_start = np.where(self.a.idx == train_index[0]) # match the date index stored in self.a.idx to the first training date\n",
    "        # TODO - figure out the data structure of temp_train_start\n",
    "        temp_index_train = [] # list of indices for training data\n",
    "\n",
    "        for i in temp_train_start[0]: # for each index in temp_train_start[0]\n",
    "            temp_index_train.extend(list(range(i, i + len(train_index)))) # add the indices for the training data\n",
    "\n",
    "        temp_predict_start = np.where(self.a.idx == predict_index[0]) # get the index of the first prediction date\n",
    "        temp_index_predict = []\n",
    "\n",
    "        for i in temp_predict_start[0]:\n",
    "            temp_index_predict.extend(list(range(i, i + len(predict_index)))) # add the indices for the prediction data\n",
    "        \n",
    "        train_x = self.a.x[temp_index_train] # get the training predictor data\n",
    "        train_y = self.a.y[temp_index_train] # get the training target data\n",
    "        test_x = self.a.x[temp_index_predict] # get the test predictor data\n",
    "        test_y = self.a.y[temp_index_predict] # get the test target data\n",
    "\n",
    "        # ! ===============================================================\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_day_range = list(range(0, 15))\n",
    "rp = RollingPredict(back_day=back_day_range, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RollingPredict' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_2.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/beneverman/Documents/Coding/QuantHive/IDVF-Oxford-v1/notebooks/linear-replication-chunk_2.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rp\u001b[39m.\u001b[39;49mx\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RollingPredict' object has no attribute 'x'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppd_agg_bars = data.shape[0] - window_length - len(back_day_range)\n",
    "expected_shape = (ppd_agg_bars * len(namelist), len(back_day_range), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558093, 15, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558093,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.a.idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(558093,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.a.idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Timestamp('2019-10-15 15:40:00+0000', tz='UTC'),\n",
       "       Timestamp('2019-10-15 16:45:00+0000', tz='UTC'),\n",
       "       Timestamp('2019-10-15 17:50:00+0000', tz='UTC'), ...,\n",
       "       Timestamp('2023-10-09 15:40:00+0000', tz='UTC'),\n",
       "       Timestamp('2023-10-09 16:45:00+0000', tz='UTC'),\n",
       "       Timestamp('2023-10-09 17:50:00+0000', tz='UTC')], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp.a.idx # this is a list of timestamps from the data df (length = num rows in data df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! deconstructing the train function\n",
    "# args\n",
    "train_index = None\n",
    "predict_index = None\n",
    "\n",
    "temp_train_start = np.where(rp.a.idx == train_index[0]) # match the date index stored in self.a.idx to the first training date\n",
    "# TODO - figure out the data structure of temp_train_start\n",
    "temp_index_train = [] # list of indices for training data\n",
    "\n",
    "for i in temp_train_start[0]: # for each index in temp_train_start[0]\n",
    "    temp_index_train.extend(list(range(i, i + len(train_index)))) # add the indices for the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TMO</th>\n",
       "      <th>ABT</th>\n",
       "      <th>HD</th>\n",
       "      <th>MCD</th>\n",
       "      <th>PG</th>\n",
       "      <th>CAT</th>\n",
       "      <th>DIS</th>\n",
       "      <th>CCI</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>KO</th>\n",
       "      <th>...</th>\n",
       "      <th>COST_logvol</th>\n",
       "      <th>IBM_logvol</th>\n",
       "      <th>ADP_logvol</th>\n",
       "      <th>AVGO_logvol</th>\n",
       "      <th>WMT_logvol</th>\n",
       "      <th>AMGN_logvol</th>\n",
       "      <th>INTU_logvol</th>\n",
       "      <th>AXP_logvol</th>\n",
       "      <th>MMC_logvol</th>\n",
       "      <th>CB_logvol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-11 13:30:00+00:00</th>\n",
       "      <td>231.4025</td>\n",
       "      <td>68.9400</td>\n",
       "      <td>195.260</td>\n",
       "      <td>165.720</td>\n",
       "      <td>80.680</td>\n",
       "      <td>143.9900</td>\n",
       "      <td>112.295</td>\n",
       "      <td>107.9300</td>\n",
       "      <td>136.72</td>\n",
       "      <td>45.3500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11 14:35:00+00:00</th>\n",
       "      <td>228.4800</td>\n",
       "      <td>68.6200</td>\n",
       "      <td>193.320</td>\n",
       "      <td>164.405</td>\n",
       "      <td>80.185</td>\n",
       "      <td>142.7600</td>\n",
       "      <td>111.960</td>\n",
       "      <td>107.0100</td>\n",
       "      <td>135.64</td>\n",
       "      <td>45.1800</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11 15:40:00+00:00</th>\n",
       "      <td>231.4800</td>\n",
       "      <td>69.3857</td>\n",
       "      <td>194.630</td>\n",
       "      <td>164.970</td>\n",
       "      <td>79.970</td>\n",
       "      <td>143.6000</td>\n",
       "      <td>112.580</td>\n",
       "      <td>107.0700</td>\n",
       "      <td>136.26</td>\n",
       "      <td>45.2600</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11 16:45:00+00:00</th>\n",
       "      <td>228.9800</td>\n",
       "      <td>68.8200</td>\n",
       "      <td>193.570</td>\n",
       "      <td>164.150</td>\n",
       "      <td>79.660</td>\n",
       "      <td>142.8800</td>\n",
       "      <td>112.190</td>\n",
       "      <td>106.7900</td>\n",
       "      <td>135.87</td>\n",
       "      <td>45.0800</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-11 17:50:00+00:00</th>\n",
       "      <td>226.8900</td>\n",
       "      <td>68.3500</td>\n",
       "      <td>190.040</td>\n",
       "      <td>162.850</td>\n",
       "      <td>79.210</td>\n",
       "      <td>141.8500</td>\n",
       "      <td>111.200</td>\n",
       "      <td>105.3590</td>\n",
       "      <td>134.41</td>\n",
       "      <td>44.7200</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-09 14:35:00+00:00</th>\n",
       "      <td>492.3400</td>\n",
       "      <td>96.0800</td>\n",
       "      <td>291.235</td>\n",
       "      <td>248.350</td>\n",
       "      <td>141.960</td>\n",
       "      <td>267.2700</td>\n",
       "      <td>83.895</td>\n",
       "      <td>92.6000</td>\n",
       "      <td>157.94</td>\n",
       "      <td>52.3700</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010464</td>\n",
       "      <td>-3.320969</td>\n",
       "      <td>-3.070882</td>\n",
       "      <td>-2.343526</td>\n",
       "      <td>-3.503360</td>\n",
       "      <td>-2.974321</td>\n",
       "      <td>-2.100127</td>\n",
       "      <td>-2.440170</td>\n",
       "      <td>-3.392075</td>\n",
       "      <td>-3.049423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-09 15:40:00+00:00</th>\n",
       "      <td>492.7300</td>\n",
       "      <td>95.8850</td>\n",
       "      <td>292.350</td>\n",
       "      <td>247.935</td>\n",
       "      <td>142.340</td>\n",
       "      <td>267.8525</td>\n",
       "      <td>83.990</td>\n",
       "      <td>92.7200</td>\n",
       "      <td>157.58</td>\n",
       "      <td>52.2300</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010567</td>\n",
       "      <td>-3.321803</td>\n",
       "      <td>-3.070941</td>\n",
       "      <td>-2.343492</td>\n",
       "      <td>-3.503311</td>\n",
       "      <td>-2.973728</td>\n",
       "      <td>-2.100285</td>\n",
       "      <td>-2.440062</td>\n",
       "      <td>-3.392489</td>\n",
       "      <td>-3.050546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-09 16:45:00+00:00</th>\n",
       "      <td>494.6027</td>\n",
       "      <td>96.4200</td>\n",
       "      <td>294.245</td>\n",
       "      <td>249.410</td>\n",
       "      <td>142.935</td>\n",
       "      <td>270.1700</td>\n",
       "      <td>84.515</td>\n",
       "      <td>93.3179</td>\n",
       "      <td>158.23</td>\n",
       "      <td>52.5250</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010431</td>\n",
       "      <td>-3.321045</td>\n",
       "      <td>-3.070520</td>\n",
       "      <td>-2.342697</td>\n",
       "      <td>-3.502854</td>\n",
       "      <td>-2.973236</td>\n",
       "      <td>-2.099906</td>\n",
       "      <td>-2.440027</td>\n",
       "      <td>-3.392360</td>\n",
       "      <td>-3.050345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-09 17:50:00+00:00</th>\n",
       "      <td>497.1050</td>\n",
       "      <td>96.7100</td>\n",
       "      <td>295.300</td>\n",
       "      <td>249.300</td>\n",
       "      <td>143.010</td>\n",
       "      <td>271.0400</td>\n",
       "      <td>84.735</td>\n",
       "      <td>93.3600</td>\n",
       "      <td>158.35</td>\n",
       "      <td>52.6108</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.010273</td>\n",
       "      <td>-3.321071</td>\n",
       "      <td>-3.072120</td>\n",
       "      <td>-2.355845</td>\n",
       "      <td>-3.503667</td>\n",
       "      <td>-2.974950</td>\n",
       "      <td>-2.101852</td>\n",
       "      <td>-2.440082</td>\n",
       "      <td>-3.396396</td>\n",
       "      <td>-3.052228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-09 18:55:00+00:00</th>\n",
       "      <td>496.0900</td>\n",
       "      <td>96.7600</td>\n",
       "      <td>295.360</td>\n",
       "      <td>249.950</td>\n",
       "      <td>143.360</td>\n",
       "      <td>271.1600</td>\n",
       "      <td>84.700</td>\n",
       "      <td>93.1800</td>\n",
       "      <td>158.54</td>\n",
       "      <td>52.8800</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.011331</td>\n",
       "      <td>-3.322267</td>\n",
       "      <td>-3.073881</td>\n",
       "      <td>-2.359830</td>\n",
       "      <td>-3.504016</td>\n",
       "      <td>-2.975468</td>\n",
       "      <td>-2.102021</td>\n",
       "      <td>-2.440440</td>\n",
       "      <td>-3.396945</td>\n",
       "      <td>-3.052782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7516 rows × 186 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                TMO      ABT       HD      MCD       PG  \\\n",
       "datetime                                                                  \n",
       "2018-10-11 13:30:00+00:00  231.4025  68.9400  195.260  165.720   80.680   \n",
       "2018-10-11 14:35:00+00:00  228.4800  68.6200  193.320  164.405   80.185   \n",
       "2018-10-11 15:40:00+00:00  231.4800  69.3857  194.630  164.970   79.970   \n",
       "2018-10-11 16:45:00+00:00  228.9800  68.8200  193.570  164.150   79.660   \n",
       "2018-10-11 17:50:00+00:00  226.8900  68.3500  190.040  162.850   79.210   \n",
       "...                             ...      ...      ...      ...      ...   \n",
       "2023-10-09 14:35:00+00:00  492.3400  96.0800  291.235  248.350  141.960   \n",
       "2023-10-09 15:40:00+00:00  492.7300  95.8850  292.350  247.935  142.340   \n",
       "2023-10-09 16:45:00+00:00  494.6027  96.4200  294.245  249.410  142.935   \n",
       "2023-10-09 17:50:00+00:00  497.1050  96.7100  295.300  249.300  143.010   \n",
       "2023-10-09 18:55:00+00:00  496.0900  96.7600  295.360  249.950  143.360   \n",
       "\n",
       "                                CAT      DIS       CCI     JNJ       KO  ...  \\\n",
       "datetime                                                                 ...   \n",
       "2018-10-11 13:30:00+00:00  143.9900  112.295  107.9300  136.72  45.3500  ...   \n",
       "2018-10-11 14:35:00+00:00  142.7600  111.960  107.0100  135.64  45.1800  ...   \n",
       "2018-10-11 15:40:00+00:00  143.6000  112.580  107.0700  136.26  45.2600  ...   \n",
       "2018-10-11 16:45:00+00:00  142.8800  112.190  106.7900  135.87  45.0800  ...   \n",
       "2018-10-11 17:50:00+00:00  141.8500  111.200  105.3590  134.41  44.7200  ...   \n",
       "...                             ...      ...       ...     ...      ...  ...   \n",
       "2023-10-09 14:35:00+00:00  267.2700   83.895   92.6000  157.94  52.3700  ...   \n",
       "2023-10-09 15:40:00+00:00  267.8525   83.990   92.7200  157.58  52.2300  ...   \n",
       "2023-10-09 16:45:00+00:00  270.1700   84.515   93.3179  158.23  52.5250  ...   \n",
       "2023-10-09 17:50:00+00:00  271.0400   84.735   93.3600  158.35  52.6108  ...   \n",
       "2023-10-09 18:55:00+00:00  271.1600   84.700   93.1800  158.54  52.8800  ...   \n",
       "\n",
       "                           COST_logvol  IBM_logvol  ADP_logvol  AVGO_logvol  \\\n",
       "datetime                                                                      \n",
       "2018-10-11 13:30:00+00:00          NaN         NaN         NaN          NaN   \n",
       "2018-10-11 14:35:00+00:00          NaN         NaN         NaN          NaN   \n",
       "2018-10-11 15:40:00+00:00          NaN         NaN         NaN          NaN   \n",
       "2018-10-11 16:45:00+00:00          NaN         NaN         NaN          NaN   \n",
       "2018-10-11 17:50:00+00:00          NaN         NaN         NaN          NaN   \n",
       "...                                ...         ...         ...          ...   \n",
       "2023-10-09 14:35:00+00:00    -3.010464   -3.320969   -3.070882    -2.343526   \n",
       "2023-10-09 15:40:00+00:00    -3.010567   -3.321803   -3.070941    -2.343492   \n",
       "2023-10-09 16:45:00+00:00    -3.010431   -3.321045   -3.070520    -2.342697   \n",
       "2023-10-09 17:50:00+00:00    -3.010273   -3.321071   -3.072120    -2.355845   \n",
       "2023-10-09 18:55:00+00:00    -3.011331   -3.322267   -3.073881    -2.359830   \n",
       "\n",
       "                           WMT_logvol  AMGN_logvol  INTU_logvol  AXP_logvol  \\\n",
       "datetime                                                                      \n",
       "2018-10-11 13:30:00+00:00         NaN          NaN          NaN         NaN   \n",
       "2018-10-11 14:35:00+00:00         NaN          NaN          NaN         NaN   \n",
       "2018-10-11 15:40:00+00:00         NaN          NaN          NaN         NaN   \n",
       "2018-10-11 16:45:00+00:00         NaN          NaN          NaN         NaN   \n",
       "2018-10-11 17:50:00+00:00         NaN          NaN          NaN         NaN   \n",
       "...                               ...          ...          ...         ...   \n",
       "2023-10-09 14:35:00+00:00   -3.503360    -2.974321    -2.100127   -2.440170   \n",
       "2023-10-09 15:40:00+00:00   -3.503311    -2.973728    -2.100285   -2.440062   \n",
       "2023-10-09 16:45:00+00:00   -3.502854    -2.973236    -2.099906   -2.440027   \n",
       "2023-10-09 17:50:00+00:00   -3.503667    -2.974950    -2.101852   -2.440082   \n",
       "2023-10-09 18:55:00+00:00   -3.504016    -2.975468    -2.102021   -2.440440   \n",
       "\n",
       "                           MMC_logvol  CB_logvol  \n",
       "datetime                                          \n",
       "2018-10-11 13:30:00+00:00         NaN        NaN  \n",
       "2018-10-11 14:35:00+00:00         NaN        NaN  \n",
       "2018-10-11 15:40:00+00:00         NaN        NaN  \n",
       "2018-10-11 16:45:00+00:00         NaN        NaN  \n",
       "2018-10-11 17:50:00+00:00         NaN        NaN  \n",
       "...                               ...        ...  \n",
       "2023-10-09 14:35:00+00:00   -3.392075  -3.049423  \n",
       "2023-10-09 15:40:00+00:00   -3.392489  -3.050546  \n",
       "2023-10-09 16:45:00+00:00   -3.392360  -3.050345  \n",
       "2023-10-09 17:50:00+00:00   -3.396396  -3.052228  \n",
       "2023-10-09 18:55:00+00:00   -3.396945  -3.052782  \n",
       "\n",
       "[7516 rows x 186 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7516, 186)\n",
      "(558093, 15, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(rp.a.x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.74680026],\n",
       "        [-2.7492632 ],\n",
       "        [-2.75039409],\n",
       "        ...,\n",
       "        [-2.76376732],\n",
       "        [-2.76364104],\n",
       "        [-2.76366267]],\n",
       "\n",
       "       [[-2.7492632 ],\n",
       "        [-2.75039409],\n",
       "        [-2.75210579],\n",
       "        ...,\n",
       "        [-2.76364104],\n",
       "        [-2.76366267],\n",
       "        [-2.7642805 ]],\n",
       "\n",
       "       [[-2.75039409],\n",
       "        [-2.75210579],\n",
       "        [-2.75336631],\n",
       "        ...,\n",
       "        [-2.76366267],\n",
       "        [-2.7642805 ],\n",
       "        [-2.76427444]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.04678476],\n",
       "        [-3.04594923],\n",
       "        [-3.04616489],\n",
       "        ...,\n",
       "        [-3.04934018],\n",
       "        [-3.049423  ],\n",
       "        [-3.05054623]],\n",
       "\n",
       "       [[-3.04594923],\n",
       "        [-3.04616489],\n",
       "        [-3.04600895],\n",
       "        ...,\n",
       "        [-3.049423  ],\n",
       "        [-3.05054623],\n",
       "        [-3.05034485]],\n",
       "\n",
       "       [[-3.04616489],\n",
       "        [-3.04600895],\n",
       "        [-3.04763715],\n",
       "        ...,\n",
       "        [-3.05054623],\n",
       "        [-3.05034485],\n",
       "        [-3.05222795]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_length = 6*250 # 250 days\n",
    "train_size = 6*1000 # 1000 days\n",
    "epoch_num = 1\n",
    "pre = True\n",
    "\n",
    "T = int(rp.a.x.shape[0]/len(namelist)) \n",
    "result_list = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
